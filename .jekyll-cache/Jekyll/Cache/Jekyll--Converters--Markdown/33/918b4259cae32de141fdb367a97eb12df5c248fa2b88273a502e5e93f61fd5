I"<p>This article is a leture note for <a href="https://www.youtube.com/watch?v=SnstUsMJ4V4&amp;list=PLF8C86C2E163D8E4E&amp;index=16" target="_blank">Lecture - 16 Introduction to Image and Video Coding</a></p>

<h2 id="intro">Intro</h2>
<p>In previous lectures we have covered various speech codings:</p>
<ul>
  <li><u>Waveform-based approach</u>: PCM, APCM, DPCM, etc.
    <ul>
      <li>In which 1-dimensional signals variable respect to time</li>
    </ul>
  </li>
  <li><u>Parametric approach</u>: LPC Analysis</li>
</ul>

<p><br />
How does image coding works?<br />
We are taking an image as a 2-dimensional array, and the signal is represented as <code class="language-plaintext highlighter-rouge">s(n1, n2)</code>, which <code class="language-plaintext highlighter-rouge">n1</code>  and <code class="language-plaintext highlighter-rouge">n2</code> are pixel indices.<br />
Instead of having variability respect to time, image has variability respect to space. <br />
<code class="language-plaintext highlighter-rouge">s(n1, n2)</code> is the sampled form of the analog 2D waveform.
<br /><br />
Then how about videos?<br />
Video could be taken as a sequence of images, which contains a 3rd dimension: time.<br />
It has variability respect to time.<br />
We could denote this as <code class="language-plaintext highlighter-rouge">s(n1, n2, k)</code>, where <code class="language-plaintext highlighter-rouge">k</code>  indicates the frame number.
<br /><br />
The fundamental bitrate in case of image and video is much higher than that of the speech signal. <br />
Therefore, image and video must be compressed to a very significant extent.</p>

<h2 id="image-coding">Image Coding</h2>
<p>The redundancy in almost all types of natural images is very high.
<br /><br />
Basically there are two types of redundancy in images:</p>
<ul>
  <li><u>Statistical redundancy</u>: The redundancy existing in space in between the image samples which are in he close neighborhood</li>
  <li><u>Phychovisual redundancy</u>: Information that is relatively unimportant to the human visual system</li>
</ul>

<p><br />
How do we make use of these types of redundancy?<br />
First suppose we have a patch of images which is represented by numbers.<br />
We could scan these numbers from the top left to the bottom right, which is called row-scan ordering.<br />
Therefore a sequence of one dimensional pixels will be available to us, and it should be possible for us to exploit the redundancy.</p>

<h4 id="a-generic-block-diagram-of-image-processing">A Generic Block Diagram of Image processing</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                --------------        -----------         -----------------
s(n1, n2)  --&gt;  | Transformer |  --&gt; | Quantizer |  --&gt;  | Entropy Encoder |
                --------------        -----------         -----------------
                   Lossless             Lossy                 Loseless
                 [Statistical]       [Psychovisual]
</code></pre></div></div>
<p>Every symbol at the quantizer output will be associated with some probability of occurence, and based on the probability of occurence we could compute the entropy.<br />
According to Shannonâ€™s entropy coding theorem, the minimum attainable bit rate that a system can have is going to be its entropy, so we cannot compress beyond its entropy value.
<br /><br />
The diagram above is called our encoder, and the decoder will do the exact reversal of this.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ------------------         --------------        -------------------- 
--&gt; | Entropy Decoder |  --&gt;  | Dequantizer |  --&gt;  | Invert Transformer |  -&gt;  s(n1, n2)
    ------------------         --------------        --------------------
</code></pre></div></div>

<h4 id="lossless-compression--lossy-compression">Lossless Compression &amp; Lossy Compression</h4>
<p>Some examples of lossless compression:</p>
<ul>
  <li><u>Huffman Coding</u>: Choose a representation for each symbol, resulting in a prefix code</li>
  <li><u>Arithmetic Coding</u>: Encode the sequence using a number</li>
  <li><u>Lempel - Ziv Coding</u></li>
</ul>

<p>An example of arithmetic coding:
<img src="../assets/img/misc/Arithmetic_coding_example.svg" alt="" /></p>

<p>Demerit of using lossless compression:</p>
<ul>
  <li>Only achieve a limited amount of compression using the lossless techniques</li>
  <li>Actually it is hard to detect the degradation in the quality of the image</li>
  <li>Accepting some lossy image compression scheme will lead to more efficiency.</li>
</ul>

<h4 id="transformation-block">Transformation Block</h4>
<p>In order to know what lossy element is, we need to dig deeper in the transformation block.
What the transformation process tends to do is to exploit the correlation that exists in the signal. This is called energy packing.
Transformer should offer some energy compaction. Energy compaction means that the energy is more concentrated using only a few spectral coefficients.</p>

<p>Two requirements for transformation:
It must be energy preserving
It must be an orthogonal transform</p>

<p>Some popular transformations:
Karhunen-Loeve transform
Discrete Fourier Transform
Discrete Cosine Transform
Discrete Wavelet Transform</p>
:ET