I"ë%<h2 id="affinity-propagation">Affinity Propagation</h2>

<h5 id="similarity-score--similarity-matrix">Similarity Score &amp; Similarity Matrix</h5>
<p>The similarity score <code class="language-plaintext highlighter-rouge">s(i, j)</code> is calculated using <code class="language-plaintext highlighter-rouge">-||xi-xj||Â²</code>
For <code class="language-plaintext highlighter-rouge">s(i, i)</code>, we could fill them in with the lowest value among all cells.</p>

<h5 id="responsibility">Responsibility</h5>
<p><img src="https://user-images.githubusercontent.com/33112694/114299242-d3b8e000-9aec-11eb-8dc6-b29968d35458.png" alt="" /></p>

<p>In this formula, <code class="language-plaintext highlighter-rouge">i</code> refers to the row and k the column of the associated matrix. (All values for a is set to zero initially)</p>

<h5 id="availability">Availability</h5>
<p>For elements in the diagonal:</p>

<p><img src="https://user-images.githubusercontent.com/33112694/114299244-d4ea0d00-9aec-11eb-84fe-8ccc5708193c.png" alt="" /></p>

<p>For other elements:</p>

<p><img src="https://user-images.githubusercontent.com/33112694/114299245-d582a380-9aec-11eb-89ce-9833b0c81c00.png" alt="" /></p>

<p>Similarly, <code class="language-plaintext highlighter-rouge">i</code> refers to the row and <code class="language-plaintext highlighter-rouge">k</code> the column of the associated matrix.</p>

<h5 id="criteria">Criteria</h5>
<p>The criteria score equals the sum of responsibility and availability:</p>

<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext">c(i, k) = r(i, k) + a(i, k)</code></pre></figure>

<p>Rows that share the same exemplar are in the same cluster.</p>

<h2 id="agglomerative-clustering">Agglomerative Clustering</h2>
<p>It is also called bottom-up clustering.</p>
<h5 id="calculate-distance">Calculate Distance</h5>
<p>A common way to calculate distance between two points is Euclidean Distance:
<img src="https://user-images.githubusercontent.com/33112694/114299247-d61b3a00-9aec-11eb-8f5b-9c7202aa88ff.png" width="240" /></p>

<p>After calculating all pair-wise distances, we merge the smallest non-zero distance in the matrix as a new node.</p>

<h5 id="linkage">Linkage</h5>
<p>There are lots of linkage strategies such as Single Linkage, Average Linkage, Complete Linkage, Ward Linkage.</p>

<ul>
  <li>In single linkage clustering, the similarity of two clusters is the similarity of their most similar members, which is the minimum distance between cluster data points.</li>
  <li>In complete-linkage clustering , the similarity of two clusters is the similarity of their most dissimilar members, which is the maximum distance between data points.</li>
  <li>In average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of data points.</li>
  <li>WardÂ´s Method seeks to choose the successive clustering steps so as to minimize the increase in ESS (Error Sum of Squares) at each step.</li>
</ul>

<p>Upon calculating the pair-wise distances, we could repeat merging and calculating the distances until all data points are merged into one cluster.</p>

<h5 id="finding-the-number-of-clusters">Finding the Number of Clusters</h5>
<p>After we get our dendrogram, we still need to choose a number of clusters since the dendrogram only demonstrates the hierarchy. Usually we could visualize the dendrogram and cut at the longest branch, or cut at branches that are longer than a specific value, but there are also some dynamic cutting methods.</p>

<h2 id="birch">BIRCH</h2>
<h5 id="clustering-feature">Clustering Feature</h5>
<p>A Clustering Feature (CF) is defined as a triplet <code class="language-plaintext highlighter-rouge">(N, LS, SS)</code>, where</p>
<ul>
  <li>N is the number of data points in the CF</li>
  <li>LS is the linear sum of data points in the CF</li>
  <li>SS is the square sum of data points in the CF</li>
</ul>

<p>For example, assume there are 5 data points (3, 4), (2, 6), (4, 5), (4, 7), (3, 8), then the corresponding CF equals (5, (16, 30), (54, 190))</p>

<p>When adding two CFs, say <code class="language-plaintext highlighter-rouge">CF1 = (N1, LS1, SS1)</code> and <code class="language-plaintext highlighter-rouge">CF2 = (N2, LS2, SS2)</code>, the sum is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CF1 + CF2 = (N1 + N2, LS1 + LS2, SS1 + SS2)
</code></pre></div></div>

<h5 id="phase-1-building-a-cf-tree">Phase 1: Building a CF Tree</h5>
<p>Before introduce how to build a CF Tree,  there are some key concepts:</p>
<ul>
  <li>Threshold (T): The maximum number of data points that a leaf node can hold</li>
  <li>Branching Factor (B): The maximum number of CF sub-clusters that a non-leaf node can hold</li>
  <li>Centroid (X0)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/33112694/114299248-d6b3d080-9aec-11eb-848d-54ed5f2bae24.png" alt="" /></li>
    </ul>
  </li>
  <li>Radius (R): The average distance from member points to the centroid
    <ul>
      <li><img src="https://user-images.githubusercontent.com/33112694/114299250-d74c6700-9aec-11eb-80ad-653c09cfce74.png" alt="" /></li>
    </ul>
  </li>
  <li>Diameter (D): The average pairwise distance within a cluster
    <ul>
      <li><img src="https://user-images.githubusercontent.com/33112694/114299252-d7e4fd80-9aec-11eb-926e-db2896e3dba2.png" alt="" /></li>
    </ul>
  </li>
</ul>

<p>To form a CF tree, we have following steps:</p>
<center>
<img src="https://user-images.githubusercontent.com/33112694/114299253-d7e4fd80-9aec-11eb-8fe2-c78f1d71f717.png" width="540" />
</center>

<ol>
  <li>Start with initial values of T and B, we keep scanning through data points. Each time when a new data point is added, we find the closest leaf node using a chosen distance criteria (e.g. Euclidean distance or Manhattan distance or something else)</li>
  <li>Upon finding the closest leaf node, we check if adding this data point to the node will violating the threshold condition
    <ul>
      <li>If it does not violate, update the CF entry</li>
      <li>Otherwise, check if a new entry could be added to the leaf
        <ul>
          <li>If thereâ€™s space for this new entry, we are done</li>
          <li>Otherwise, split the leaf node by choosing the farthest pair of entries as seeds and redistributing the remaining entries based on the distance criteria</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>After inserting the data point, we update the CF information for each non-leaf node. In the event of a split, we must insert a new non-leaf node into the parent node and have it point to the newly formed leaf. If the parent has space for new nodes, we only need to update CF vectors, but generally we may need to split the non-leaf parent nodes as well</li>
</ol>

<h5 id="phase-2-condense-the-tree">Phase 2: Condense the Tree</h5>
<center>
<img src="https://user-images.githubusercontent.com/33112694/114299256-d9162a80-9aec-11eb-841c-6020680b0001.png" width="450" />
</center>

<p>Although it is optional, phase 2 may work as a cushion to rebuild a smaller CF tree, removing more outliers and grouping crowded sub-clusters into larger ones.</p>
<h5 id="phase-3-global-clustering">Phase 3: Global Clustering</h5>
<p>Due to skewed input order and splitting triggered by page size, we may use a global or semi-global algorithm to cluster all leaf nodes. Almost any existing clustering algorithms could be adapted to work with sub-clusters. For sub-clusters, we can:</p>
<ol>
  <li>Use the centroid as the representative of each cluster, treat each sub-cluster as a single point and use an existing algorithm without modification</li>
  <li>Treat a sub-cluster of n data points as its centroid repeating n times and modifying an existing algorithm slightly to take the counting information into account</li>
  <li>Directly apply an existing algorithm to sub-clusters using CF vectors</li>
</ol>

<h5 id="phase-4-cluster-refining">Phase 4: Cluster Refining</h5>
<p>Phase 4 utilizes centroids of clusters produced by phase 3, and redistributes data points to the closest centroids. It guarantees all copies of a given data point go to the same cluster, correcting the inaccuracy caused by the algorithm. Besides, phase 4 could also discard outliers.</p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://scikit-learn.org/stable/modules/clustering.html">2.3. Clustering â€” scikit-learn 0.24.1 documentation</a></li>
  <li><a href="https://towardsdatascience.com/unsupervised-machine-learning-affinity-propagation-algorithm-explained-d1fef85f22c8">Affinity Propagation Algorithm Explained</a></li>
  <li><a href="https://towardsdatascience.com/breaking-down-the-agglomerative-clustering-process-1c367f74c7c2">Breaking down the agglomerative clustering process</a></li>
  <li><a href="https://www.statistics.com/glossary/wards-linkage/">WardÂ´s Linkage - Statistics.com: Data Science, Analytics &amp; Statistics Courses</a></li>
  <li><a href="https://academic.oup.com/bioinformatics/article/24/5/719/200751">Defining clusters from a hierarchical cluster tree: the Dynamic Tree Cut package for R</a></li>
  <li><a href="https://towardsdatascience.com/machine-learning-birch-clustering-algorithm-clearly-explained-fb9838cbeed9">BIRCH Clustering Algorithm Example In Python</a></li>
  <li><a href="https://morioh.com/p/c23e0d680669">BIRCH Clustering Clearly Explained - Principle of BIRCH clustering algorithm</a></li>
  <li><a href="https://www2.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">BIRCH: An efficient data clustering method for large databases</a></li>
</ul>
:ET