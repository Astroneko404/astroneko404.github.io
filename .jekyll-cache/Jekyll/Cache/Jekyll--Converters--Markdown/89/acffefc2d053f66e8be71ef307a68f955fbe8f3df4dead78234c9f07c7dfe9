I"œ<h2 id="dbscan">DBSCAN</h2>
<p>DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise, which is presented in the paper <em>A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</em>.</p>

<p>It requires two parameters:</p>
<ul>
  <li>eps (epsilon): The maximum distance between two data points to be considered as a cluster;</li>
  <li>minPts: The minimum number of points to form a dense region.</li>
</ul>

<p>DBSCAN works as below:</p>
<ol>
  <li>Arbitrarily choose a data point in the dataset (until all points have been visited);</li>
  <li>Check whether there are at least <code class="language-plaintext highlighter-rouge">minPts</code> points within the area of radius <code class="language-plaintext highlighter-rouge">eps</code>. If yes, consider all of them to be part of the same cluster.</li>
</ol>

<p>Unlike K-Means, DBSCAN does not need to specify the number of clusters. Besides, since K-Means forms clusters based on the mean value of cluster points, a slight change in data points might affect the outcome.</p>

<p>However, it is challenging for DBSCAN to deal with data points with unbalanced density due to the fixed epsilon.</p>

<h2 id="k-means">K-Means</h2>
<p>Basically, given <code class="language-plaintext highlighter-rouge">k</code> as the number of centroids, K-means algorithm allocates every data point to the nearest cluster, while keeping the centroids as small as possible.</p>
<ol>
  <li>Randomly choose <code class="language-plaintext highlighter-rouge">k</code> data points as centroids</li>
  <li>For every other data points in the dataset, assign them to the closest centroid based on a specific distance metric (e.g. Euclidean distance)</li>
  <li>Re-calculate the centroid of each cluster</li>
  <li>Repeat the iteration until condition met</li>
</ol>

<h4 id="how-to-determine-the-optimal-value-of-k">How to Determine the Optimal Value of K</h4>
<h6 id="i-the-elbow-method">I. The Elbow Method</h6>
<blockquote>
  <p>Calculate the <u>Within-Cluster-Sum of Squared Errors (WSS)</u> for different values of k, and choose the k for which WSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow.</p>
</blockquote>

<p>The within-cluster sum of squares demonstrates the variability within each cluster. A cluster with smaller WSS values is more compact and has less variability.</p>

<p>To Calculate WSS, we calculate <u>Squared Error</u>, which is the square of the distance between every data point and it‚Äôs corresponding centroid, then sum up all Squared Errors as WSS.</p>

<p>For example, in the following figure, the elbow is at k=4.</p>

<center>
<img src="https://user-images.githubusercontent.com/33112694/114336362-568b7a80-9b81-11eb-9f2b-122a6a41ad40.JPG" />
</center>

<p>However, the curve could be much more ambiguous than this one.</p>

<h6 id="ii-gap-static">II. Gap Static</h6>
<p>In the paper <em>Estimating the number of clusters in a data set via the gap statistic</em>, Tibshirani et al. presented a method, that is called gap static:</p>
<ul>
  <li>Cluster the data and denote the number of clusters as k=1, ‚Ä¶, k<sub>max</sub></li>
  <li>Generate B reference data sets with a random uniform distribution. Cluster each of these reference data sets with varying number of clusters k=1, ‚Ä¶, k<sub>max</sub>, and compute the corresponding total within intra-cluster variation W<sub>kb</sub></li>
  <li>Compute the estimated gap statistic as the deviation of the observed W<sub>k</sub> value from its expected value W<sub>kb</sub> under the null hypothesis:</li>
</ul>

<center>
<img src="https://user-images.githubusercontent.com/33112694/114491105-95d0ce80-9c48-11eb-84c4-d0fea0bc0d09.jpg" width="300" />
</center>

<ul>
  <li>Choose the number of clusters as the smallest value of k such that:</li>
</ul>

<center>
<img src="https://user-images.githubusercontent.com/33112694/114491355-18598e00-9c49-11eb-9e15-29f09e4dcb99.jpg" width="280" />
</center>

<h5 id="iii-the-silhouette-method">III. The Silhouette Method</h5>
<p>Basically, the silhouette method</p>
<blockquote>
  <p>Compute silhouette coefficients for each of point, and average it out for all the samples to get the silhouette score.</p>
</blockquote>

<p>To compute silhouette coefficient:</p>
<ul>
  <li>Compute a(i), the mean distance between i and all other data points in the same cluster:</li>
</ul>

<center>
<img src="https://user-images.githubusercontent.com/33112694/114692252-0bbd5e80-9d4b-11eb-8b84-33592e0b5097.jpg" width="250" />
</center>

<ul>
  <li>Compute b(i), the minimum mean distance of i to all points in any other cluster, of which i is not a member</li>
</ul>

<center>
<img src="https://user-images.githubusercontent.com/33112694/114692256-0c55f500-9d4b-11eb-86b0-9a2f2269c147.jpg" />
</center>

<ul>
  <li>Compute the silhouette coefficient:</li>
</ul>

<center>
<img src="https://user-images.githubusercontent.com/33112694/114691408-3a870500-9d4a-11eb-8e15-84b45931f34a.jpg" />
</center>

<h2 id="k-means-1">K-Means++</h2>
<p>K-Means++ adds a initialization on choosing starter centroids:</p>
<ul>
  <li>Randomly choose the first centroid from data points;</li>
  <li>For each data point, calculate the distance between it and it‚Äôs corresponding centroid (the nearest one);</li>
  <li>Use the distance as a probability, that the larger distance a point has, the larger probability that this point will be selected as the next centroid;</li>
  <li>Repeat step 2 and 3 until k centroids are selected.</li>
</ul>

<h2 id="mini-batch-k-means">Mini Batch K-Means</h2>
<p>Mini Batch K-Means is almost similar to K-Means:</p>
<ol>
  <li>Randomly choose b samples from dataset to form a mini-batch;</li>
  <li>Assign them to the closest centroid;</li>
  <li>Calculate the streaming average of the sample and all previous samples assigned to that centroid;</li>
  <li>Repeat step 2 and 3 until certain condition is met.</li>
</ol>

<p>Using mini batches would save lots of time compared to K-Means, as not all data points would be included to calculate centroids in each iterations. However, sampling may also reduce the accuracy.</p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html">DBSCAN Clustering Algorithm in Machine Learning</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/78798251">„ÄêÊú∫Âô®Â≠¶‰π†„ÄëK-meansÔºàÈùûÂ∏∏ËØ¶ÁªÜÔºâ</a></li>
  <li><a href="https://scikit-learn.org/stable/modules/clustering.html">2.3. Clustering - scikit-learn 0.24.1 documentation</a></li>
  <li><a href="https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb">How to Determine the Optimal K for K-Means?</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">Elbow method (clustering)</a></li>
  <li><a href="https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/cluster-k-means/interpret-the-results/all-statistics-and-graphs/">Interpret all statistics and graphs for Cluster K-Means</a></li>
  <li><a href="https://statweb.stanford.edu/~gwalther/gap">Estimating the number of clusters in a data set via the gap statistic</a>
<a href="https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/">Determining The Optimal Number Of Clusters: 3 Must Know Methods</a></li>
  <li><a href="https://towardsdatascience.com/silhouette-method-better-than-elbow-method-to-find-optimal-clusters-378d62ff6891">Silhouette Method - Better than Elbow Method to find Optimal Clusters</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">Silhouette (clustering)</a></li>
  <li><a href="https://www.geeksforgeeks.org/ml-k-means-algorithm/">ML - K-means++ Algorithm - GeeksforGeeks</a></li>
</ul>
:ET